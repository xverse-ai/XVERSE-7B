<div align="center">
<h1>
  XVERSE-7B
</h1>
</div>

<p align="center">
        <a href="https://huggingface.co/xverse/XVERSE-7B">ğŸ¤— XVERSE-7B</a>&nbspï½œ&nbsp<a href="https://huggingface.co/xverse/XVERSE-7B-Chat">ğŸ¤— XVERSE-7B-Chat</a>&nbspï½œ&nbsp
        <a href="https://modelscope.cn/organization/xverse" rel="nofollow"><img src="resources/modelscope.png" width="20px" style="max-width: 100%;"> ModelScope</a>&nbspï½œ&nbsp
        <a href="resources/wechat.png">ğŸ’¬ å¾®ä¿¡ç¤¾åŒº</a>
</p>

<h4 align="left">
    <p>
        <a href="README.md">ä¸­æ–‡</a> |
        <b>English</b>
    <p>
</h4>

## Model Introduction

**XVERSE-7B** is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. The models released this time include the base model **XVERSE-7B** and the chat model **XVERSE-7B-Chat**. Its key features are as follows:

- **Model Structure**: XVERSE-7B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
- **Training Data**: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
- **Tokenization**: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
- **Training Framework**: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.

## Model Evaluation

In order to validate the various abilities of the model, we have chosen several comprehensive capability benchmarks across multiple disciplines, including [MMLU](https://arxiv.org/abs/2009.03300) (English), [C-Eval](https://cevalbenchmark.com/) (Chinese), [AGIEval](https://arxiv.org/abs/2304.06364) (Chinese and English), [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) (Chinese and English), [GAOKAO-English](https://github.com/ExpressAI/AI-Gaokao) (English), the evaluation results are as follows (the bolded score represent the best performances):
|       Models       |    Type    |       MMLU       |      C-Eval      | AGIEval<sup>1</sup> | GAOKAO-Bench<sup>1</sup> | GAOKAO-English<sup>1</sup> |
| :----------------: | :--------: | :--------------: | :--------------: | :-----------------: | :----------------------: | :------------------------: |
|    Baichuan-7B     | pretrained | 42.3<sup>2</sup> | 42.8<sup>2</sup> |  34.4<sup>2</sup>   |     36.3<sup>2</sup>     |            44.3            |
| Baichuan2-7B-Base  | pretrained | 54.2<sup>2</sup> | 54.0<sup>2</sup> |  42.7<sup>2</sup>   |     47.5<sup>2</sup>     |            53.1            |
| Baichuan2-7B-Chat  | fine-tuned |       53.2       |       52.2       |        41.3         |           49.7           |            66.6            |
|    ChatGLM2-6B     | fine-tuned | 45.5<sup>2</sup> | 50.1<sup>2</sup> |        42.6         |           54.2           |            59.7            |
|     Falcon-7B      | pretrained | 27.8<sup>2</sup> |       25.8       |        26.2         |           26.3           |            29.9            |
|    InternLM-7B     | pretrained | 51.0<sup>2</sup> |       52.4       |        34.1         |           53.6           |            32.3            |
|  InternLM-7B-Chat  | fine-tuned | 50.8<sup>2</sup> |       52.8       |        39.0         |         **67.4**         |            43.9            |
|      Llama-7B      | pretrained | 35.1<sup>2</sup> |       27.0       |        27.4         |           26.0           |            30.1            |
|     Llama-2-7B     | pretrained | 45.3<sup>2</sup> |       28.9       |        27.0         |           27.8           |            47.8            |
|       MPT-7B       | pretrained | 29.6<sup>2</sup> |       27.8       |        24.2         |           25.3           |            28.1            |
|   Vicuna-7B-v1.5   | fine-tuned | 49.8<sup>2</sup> |       22.9       |        26.7         |           24.4           |            61.1            |
|   **XVERSE-7B**    | pretrained |       56.6       |     **57.1**     |        46.9         |           61.7           |            71.1            |
| **XVERSE-7B-Chat** | fine-tuned |     **63.7**     |       55.4       |      **48.9**       |           57.5           |          **78.2**          |

> <sup>1: Tests are conducted only on single-answer multiple-choice questions, thus excluding fill-in-the-blanks, open-ended questions, and multiple-answer multiple-choice questions.</sup>   
> <sup>2: Reporting results from official results of each model.</sup>   
>
> For MMLU, we adopt the [evaluation tools](https://github.com/hendrycks/test) provided by the authors, C-Eval, AGIEval, GAOKAO-Bench, GAOKAO-English are the same as MMLU, and uniformly use **5-shot** to construct the test samples.

### MMLU Category Results
|       Models       |    Type    | Average  |   STEM   | Social Science | Humanities |  Others  |
| :----------------: | :--------: | :------: | :------: | :------------: | :--------: | :------: |
|    Baichuan-7B     | pretrained |   42.3   |   35.6   |      48.9      |    38.4    |   48.1   |
| Baichuan2-7B-Chat  | fine-tuned |   53.2   |   43.1   |      59.1      |    50.0    |   59.1   |
|    ChatGLM2-6B     | fine-tuned |   45.5   |   40.1   |      51.6      |    41.2    |   51.2   |
|    InternLM-7B     | pretrained |   51.0   | **58.7** |      43.5      |    52.7    |   53.2   |
|      LLaMA-7B      | pretrained |   35.1   |   30.5   |      38.3      |    34.0    |   38.1   |
|     LLaMA2-7B      | pretrained |   45.3   |   36.4   |      51.2      |    42.9    |   52.2   |
|   **XVERSE-7B**    | pretrained |   56.6   |   45.6   |      65.3      |    50.4    |   65.5   |
| **XVERSE-7B-Chat** | fine-tuned | **63.7** |   51.7   |    **72.5**    |  **58.2**  | **72.2** |

### C-Eval Category Results
|       Models       |    Type    | Average  |   STEM   | Social Science | Humanities |  Others  |
| :----------------: | :--------: | :------: | :------: | :------------: | :--------: | :------: |
|    Baichuan-7B     | pretrained |   42.8   |   38.2   |      52.0      |    46.2    |   39.3   |
| Baichuan2-7B-Base  | pretrained |   54.9   |   47.9   |      67.3      |    58.4    |   52.8   |
| Baichuan2-7B-Chat  | fine-tuned |   52.2   |   44.6   |      65.0      |    55.8    |   50.9   |
|    ChatGLM2-6B     | fine-tuned |   50.1   |   46.4   |      60.4      |    50.6    |   46.9   |
|     Falcon-7B      | pretrained |   25.8   |   25.8   |      26.0      |    25.8    |   25.7   |
|    InternLM-7B     | pretrained |   52.4   |   47.0   |      64.9      |    55.6    |   47.6   |
|  InternLM-7B-Chat  | fine-tuned |   52.8   |   48.4   |      65.6      |    57.0    |   45.0   |
|      LLaMA-7B      | pretrained |   27.0   |   26.7   |      26.7      |    28.4    |   26.2   |
|     LLaMA2-7B      | pretrained |   28.9   |   26.8   |      34.5      |    30.0    |   26.4   |
|       MPT-7B       | pretrained |   27.8   |   27.4   |      29.8      |    26.9    |   27.7   |
|   Vicuna-7B-v1.5   | fine-tuned |   22.9   |   21.8   |      23.3      |    24.0    |   23.3   |
|   **XVERSE-7B**    | pretrained | **57.1** | **48.9** |    **71.0**    |  **59.7**  | **56.7** |
| **XVERSE-7B-Chat** | fine-tuned |   55.4   |   47.9   |      68.5      |    57.3    |   55.1   |

## Usage

### Environment Setup

1. Clone this repository:

```shell
git clone https://github.com/xverse-ai/XVERSE-7B
cd XVERSE-7B
```

2. Install the dependencies using pip:

```shell
pip install -r requirements.txt
```

### Loading with Transformers

The XVERSE-7B model can be loaded to chat for inference using the following code:

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> from transformers.generation.utils import GenerationConfig
>>> model_path = "xverse/XVERSE-7B-Chat"
>>> tokenizer = AutoTokenizer.from_pretrained(model_path)
>>> model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
>>> model.generation_config = GenerationConfig.from_pretrained(model_path)
>>> model = model.eval()
>>> history = [{"role": "user", "content": "1955å¹´è°æ˜¯ç¾å›½æ€»ç»Ÿï¼Ÿä»–æ˜¯ä»€ä¹ˆå…šæ´¾ï¼Ÿ"}]
>>> response = model.chat(tokenizer, history)
>>> print(response)
1955å¹´çš„ç¾å›½æ€»ç»Ÿæ˜¯å¾·æ€€ç‰¹Â·DÂ·è‰¾æ£®è±ªå¨å°”ï¼Œä»–æ˜¯å…±å’Œå…šçš„æˆå‘˜ã€‚
>>> history.append({"role": "assistant", "content": response})
>>> history.append({"role": "user", "content": "ä»–ä»»èŒäº†å¤šå°‘å¹´"})
>>> response = model.chat(tokenizer, history)
>>> print(response)
å¾·æ€€ç‰¹Â·DÂ·è‰¾æ£®è±ªå¨å°”åœ¨1953å¹´åˆ°1961å¹´æ‹…ä»»ç¾å›½æ€»ç»Ÿï¼Œæ‰€ä»¥ä»–ä»»èŒäº†8å¹´ã€‚
```

### Web Demo

The following code can be used to start a web server. By entering the access address in the browser, you can perform inference with the XVERSE-7B model:

```shell
python chat_demo.py --port='port' --model_path='/path/to/model/' --tokenizer_path='/path/to/tokenizer/'
```

### XVERSE-7B-Chat Output Example
Below are some examples of XVERSE-7B-Chat obtained using `chat_demo.py`:

<details><summary><b>Role Play</b></summary>

```
ç”¨æˆ·:
æ‰®æ¼”æ¨è¿‡

XVERSE-7B-Chat:
å¥½çš„ï¼Œæˆ‘å°†æ‰®æ¼”æ¨è¿‡ã€‚ç°åœ¨ä½ å¯ä»¥å’Œæˆ‘å¼€å§‹å¯¹è¯äº†ã€‚

ç”¨æˆ·:
ä½ æ˜¯è°

XVERSE-7B-Chat:
æˆ‘æ˜¯æ¨è¿‡ï¼Œå°é¾™å¥³çš„å¾’å¼Ÿï¼Œä¹Ÿæ˜¯ç¥é›•å¤§ä¾ ã€‚

ç”¨æˆ·:
ä½ çš„å¦»å­æ˜¯è°

XVERSE-7B-Chat:
æˆ‘çš„å¦»å­æ˜¯å°é¾™å¥³ã€‚

ç”¨æˆ·:
é€€å‡ºè§’è‰²æ‰®æ¼”

XVERSE-7B-Chat:
å½“ç„¶ï¼Œå¦‚æœä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶é—®æˆ‘å°±è¡Œã€‚

ç”¨æˆ·:
ä½ æ˜¯è°

XVERSE-7B-Chat:
æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ç­‰ç­‰ã€‚
```
</details>

<details><summary><b>Quiz</b></summary>

```
ç”¨æˆ·:
ã€Šäººç±»ç®€å²ã€‹è¿™æœ¬ä¹¦çš„ä½œè€…æ˜¯è°ï¼Ÿè¯·ç®€è¦æ¦‚æ‹¬ä¸€ä¸‹è¿™æœ¬ä¹¦çš„å†…å®¹

XVERSE-7B-Chat:
ã€Šäººç±»ç®€å²ã€‹è¿™æœ¬ä¹¦çš„ä½œè€…æ˜¯ä»¥è‰²åˆ—å†å²å­¦å®¶å°¤ç“¦å°”Â·èµ«æ‹‰åˆ©ã€‚è¿™æœ¬ä¹¦æ˜¯ä¸€éƒ¨å…³äºäººç±»å†å²çš„é€šä¿—è¯»ç‰©ï¼Œå®ƒä»å¤§çˆ†ç‚¸å¼€å§‹è®²èµ·ï¼Œä¸€ç›´åˆ°ç°ä»£ç§‘æŠ€å¯¹äººç±»ç¤¾ä¼šçš„å½±å“ï¼Œå…¨æ–¹ä½åœ°æ­ç¤ºäº†äººç±»å†å²çš„å‘å±•è„‰ç»œå’Œæ¼”å˜è¿‡ç¨‹ã€‚

åœ¨ã€Šäººç±»ç®€å²ã€‹ä¸­ï¼Œå°¤ç“¦å°”Â·èµ«æ‹‰åˆ©æå‡ºäº†è®¸å¤šç‹¬ç‰¹è€Œæ·±åˆ»çš„è§‚ç‚¹ï¼Œå¦‚â€œå†å²çš„ç»ˆç»“â€ç†è®ºã€â€œäººç±»â€çš„å®šä¹‰ä»¥åŠâ€œäººç±»â€ä¸å…¶ä»–ç”Ÿç‰©çš„åŒºåˆ«ç­‰ã€‚ä»–è®¤ä¸ºï¼Œäººç±»å¹¶éç”Ÿæ¥å°±æœ‰è‡ªæˆ‘æ„è¯†ï¼Œè€Œæ˜¯ç”±äºè¯­è¨€çš„å‡ºç°ï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå°†è‡ªå·±çš„æƒ³æ³•ä¼ é€’ç»™ä»–äººï¼Œä»è€Œå½¢æˆå…±åŒä½“ï¼Œè¿›è€Œäº§ç”Ÿäº†æ–‡æ˜ã€‚åŒæ—¶ï¼Œä»–è¿˜æŒ‡å‡ºï¼Œç§‘æŠ€çš„å‘å±•è™½ç„¶å¸¦æ¥äº†è¯¸å¤šå¥½å¤„ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´äººç±»å¤±å»å¯¹è‡ªç„¶ç•Œçš„æ§åˆ¶ï¼Œç”šè‡³å¯èƒ½å¼•å‘ä¸€åœºå…¨çƒæ€§çš„å¤§ç¾éš¾ã€‚
```
</details>

<details><summary><b>Text Generation</b></summary>

```
ç”¨æˆ·:
æˆ‘è¦å»æ·±åœ³æ—…æ¸¸ï¼Œå¸®æˆ‘åˆ¶å®šä¸€ä»½5æ—¥æ¸¸è®¡åˆ’

XVERSE-7B-Chat:
ä»¥ä¸‹æ˜¯ä¸€ä»½ä¸ºæœŸäº”å¤©çš„æ·±åœ³æ—…æ¸¸è¡Œç¨‹å®‰æ’ï¼š

ç¬¬ä¸€å¤©ï¼šæ·±åœ³å¸‚åŒºæ¸¸
ä¸Šåˆï¼šå‚è§‚æ·±åœ³åšç‰©é¦†ï¼ˆå…è´¹ï¼‰ï¼Œäº†è§£æ·±åœ³çš„å†å²å’Œæ–‡åŒ–ã€‚
ä¸­åˆï¼šåœ¨åå¼ºåŒ—ç¾é£Ÿè¡—å“å°å„ç§åœ°é“çš„å¹¿ä¸œç¾é£Ÿã€‚
ä¸‹åˆï¼šå‰å¾€ä¸–ç•Œä¹‹çª—ï¼ˆé—¨ç¥¨çº¦180å…ƒï¼‰ï¼Œæ¬£èµå…¨çƒå„åœ°åèƒœå¤è¿¹çš„ç¼©å°ç‰ˆæ¨¡å‹ã€‚
æ™šä¸Šï¼šé€›æ·±åœ³æ¹¾å…¬å›­ï¼Œæ¬£èµæ·±åœ³æ¹¾è·¨æµ·å¤§æ¡¥å¤œæ™¯ã€‚

ç¬¬äºŒå¤©ï¼šä¸œéƒ¨åä¾¨åŸæ¸¸
ä¸Šåˆï¼šå‚è§‚å¤§æ¢…æ²™æµ·æ»¨å…¬å›­ï¼Œäº«å—é˜³å…‰ã€æ²™æ»©å’Œæµ·æµªã€‚
ä¸­åˆï¼šåœ¨é™„è¿‘çš„é¤å…ç”¨åˆé¤ã€‚
ä¸‹åˆï¼šå‰å¾€ä¸œéƒ¨åä¾¨åŸï¼ˆé—¨ç¥¨çº¦90å…ƒï¼‰ï¼Œä½“éªŒèŒ¶æºªè°·çš„èŒ¶æ–‡åŒ–ï¼Œå‚è§‚å¤§ä¾ è°·çš„è‡ªç„¶é£å…‰ã€‚
æ™šä¸Šï¼šè¿”å›å¸‚åŒºï¼Œåœ¨æ¬¢ä¹æµ·å²¸äº«å—å¤œç”Ÿæ´»ã€‚

ç¬¬ä¸‰å¤©ï¼šæ·±åœ³é‡ç”ŸåŠ¨ç‰©å›­æ¸¸
ä¸Šåˆï¼šå‰å¾€æ·±åœ³é‡ç”ŸåŠ¨ç‰©å›­ï¼ˆé—¨ç¥¨çº¦70å…ƒï¼‰ï¼Œä¸å„ç§çç¨€åŠ¨ç‰©äº²å¯†æ¥è§¦ã€‚
ä¸­åˆï¼šåœ¨åŠ¨ç‰©å›­å†…çš„é¤å…ç”¨åˆé¤ã€‚
ä¸‹åˆï¼šå‚è§‚æµ·æ´‹ç‹å›½ï¼ˆé—¨ç¥¨çº¦230å…ƒï¼‰ï¼Œè§‚çœ‹ç²¾å½©çš„æµ·è±šè¡¨æ¼”å’Œæµ·ç‹®è¡¨æ¼”ã€‚
æ™šä¸Šï¼šè¿”å›å¸‚åŒºï¼Œè‡ªç”±æ´»åŠ¨ã€‚

ç¬¬å››å¤©ï¼šæ·±åœ³å¤§å­¦åŠå‘¨è¾¹æ¸¸
ä¸Šåˆï¼šå‚è§‚æ·±åœ³å¤§å­¦ï¼ˆå…è´¹ï¼‰ï¼Œæ„Ÿå—æ ¡å›­çš„å­¦æœ¯æ°›å›´ã€‚
ä¸­åˆï¼šåœ¨é™„è¿‘çš„é¤å…ç”¨åˆé¤ã€‚
ä¸‹åˆï¼šå‰å¾€å—å±±æ–‡åŒ–ç”Ÿæ€å›­ï¼ˆé—¨ç¥¨çº¦40å…ƒï¼‰ï¼Œæ¬£èµå—å±±çš„è‡ªç„¶é£å…‰å’Œäººæ–‡æ™¯è§‚ã€‚
æ™šä¸Šï¼šè¿”å›å¸‚åŒºï¼Œè‡ªç”±æ´»åŠ¨ã€‚

ç¬¬äº”å¤©ï¼šè´­ç‰©ä¼‘é—²æ¸¸
ä¸Šåˆï¼šå‰å¾€æ·±åœ³å›½é™…è´­ç‰©ä¸­å¿ƒæˆ–åæ¶¦ä¸‡è±¡åŸç­‰å¤§å‹è´­ç‰©ä¸­å¿ƒè´­ç‰©ã€‚
ä¸­åˆï¼šåœ¨è´­ç‰©ä¸­å¿ƒå†…çš„é¤å…ç”¨åˆé¤ã€‚
ä¸‹åˆï¼šåœ¨è´­ç‰©ä¸­å¿ƒç»§ç»­è´­ç‰©æˆ–è€…ä¼‘æ¯ã€‚
æ™šä¸Šï¼šè¿”å›é…’åº—ä¼‘æ¯ï¼Œå‡†å¤‡ç¦»å¼€æ·±åœ³ã€‚
ä»¥ä¸Šå°±æ˜¯ä¸ºæ‚¨åˆ¶å®šçš„æ·±åœ³äº”æ—¥æ¸¸è®¡åˆ’ï¼Œå¸Œæœ›å¯¹æ‚¨çš„æ—…è¡Œæœ‰æ‰€å¸®åŠ©ã€‚
```
</details>

<details><summary><b>Programming Ability</b></summary>

```
ç”¨æˆ·:
å†™ä¸€ä¸ªå¿«é€Ÿæ’åºä»£ç 

XVERSE-7B-Chat:
è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨Pythonç¼–å†™çš„å¿«é€Ÿæ’åºçš„ç®€å•ç¤ºä¾‹ï¼š

def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)

print(quicksort([3,6,8,10,1,2,1]))
# Output: [1, 1, 2, 3, 6, 8, 10]
è¿™ä¸ªå‡½æ•°é¦–å…ˆæ£€æŸ¥åˆ—è¡¨çš„é•¿åº¦æ˜¯å¦å°äºæˆ–ç­‰äº1ã€‚å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå®ƒå°±è¿”å›è¯¥åˆ—è¡¨ã€‚ç„¶åï¼Œå®ƒé€‰æ‹©åˆ—è¡¨ä¸­é—´çš„å…ƒç´ ä½œä¸ºæ¢è½´ã€‚æ¥ç€ï¼Œå®ƒåˆ›å»ºä¸‰ä¸ªç©ºåˆ—è¡¨ï¼šleftã€middleå’Œrightã€‚leftåŒ…å«æ‰€æœ‰å°äºæ¢è½´çš„å…ƒç´ ï¼ŒmiddleåŒ…å«æ‰€æœ‰ç­‰äºæ¢è½´çš„å…ƒç´ ï¼Œè€ŒrightåŒ…å«æ‰€æœ‰å¤§äºæ¢è½´çš„å…ƒç´ ã€‚æœ€åï¼Œå®ƒé€’å½’åœ°å¯¹å·¦å³ä¸¤éƒ¨åˆ†è¿›è¡ŒåŒæ ·çš„æ“ä½œï¼Œå¹¶å°†ç»“æœè¿æ¥èµ·æ¥ã€‚
```
</details>

<details><summary><b>Math Ability</b></summary>

```
ç”¨æˆ·:
è©¹å§†æ–¯å†³å®šæ¯å‘¨è·‘ 3 æ¬¡ 3 æ®µå†²åˆºè·‘ï¼Œæ¯æ®µè·‘ 60 ç±³ã€‚ä»–æ¯å‘¨ä¸€å…±è·‘å¤šå°‘ç±³ï¼Ÿ

XVERSE-7B-Chat:
æ¯æ¬¡å†²åˆºè·‘æ˜¯ 60 ç±³ x 3 = 180 ç±³ã€‚
æ‰€ä»¥ï¼Œæ¯å‘¨ä»–ä¼šè·‘ 180 ç±³ x 3 = 540 ç±³ã€‚
```
</details>

<details><summary><b>Logical Reasoning</b></summary>

```
ç”¨æˆ·:
é™¨çŸ³ä¸ºä»€ä¹ˆä¼šè½åˆ°é™¨çŸ³å‘é‡Œ

XVERSE-7B-Chat:
å½“ä¸€é¢—é™¨çŸ³ä»¥æé«˜çš„é€Ÿåº¦æ’å‡»åœ°çƒæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿå·¨å¤§çš„åŠ¨èƒ½ã€‚è¿™ç§åŠ¨èƒ½åœ¨æ’å‡»ç‚¹å¤„è½¬åŒ–ä¸ºçƒ­é‡å’Œå‹åŠ›ï¼Œå¯¼è‡´é™¨çŸ³ç¬é—´èåŒ–æˆ–çˆ†ç‚¸ã€‚

å¦‚æœé™¨çŸ³çš„å¤§å°è¶³å¤Ÿå¤§ï¼Œå®ƒå¯èƒ½ä¼šç©¿é€åœ°è¡¨å¹¶è¿›å…¥ä¸‹é¢çš„åœ°å±‚ã€‚è¿™å°†ä½¿å…¶ç»§ç»­å‘åœ°å¿ƒç§»åŠ¨ï¼Œç›´åˆ°æœ€ç»ˆåœ¨åœ°å£³æˆ–åœ°å¹”ä¸­åœæ­¢ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œé™¨çŸ³ä¼šé€æ¸å†·å´å¹¶å‡å›ºï¼Œå½¢æˆä¸€ä¸ªå†…éƒ¨å……æ»¡å²©çŸ³ç¢ç‰‡çš„ç©ºæ´ã€‚

ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å¤§çš„é™¨çŸ³ä¹Ÿæ— æ³•å®Œå…¨å¡«æ»¡è¿™æ ·ä¸€ä¸ªç©ºæ´ã€‚å› æ­¤ï¼Œå½“ä¸‹ä¸€æ¬¡é™¨çŸ³æ’å‡»å‘ç”Ÿæ—¶ï¼Œè¿™äº›æœªè¢«å¡«æ»¡çš„ç©ºæ´å°±æœ‰å¯èƒ½å†æ¬¡è¢«å‡»ä¸­ï¼Œä»è€Œå½¢æˆæ–°çš„é™¨çŸ³å‘ã€‚

æ­¤å¤–ï¼Œå¦‚æœåœ°çƒä¸Šçš„åœ°å£³æ¿å—è¿åŠ¨ï¼ˆå¦‚åœ°éœ‡ã€ç«å±±æ´»åŠ¨ç­‰ï¼‰ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´å·²æœ‰çš„é™¨çŸ³å‘é‡æ–°éœ²å‡ºåœ°è¡¨ã€‚
```
</details>

<details><summary><b>Language Understanding</b></summary>

```
ç”¨æˆ·:
ç»™å‡ºä¸€ä¸ªå¥å­ä»¥åŠè¯¥å¥å­ä¸­å‡ºç°çš„ä¸¤ä¸ªå‘½åå®ä½“ï¼Œè¯·åˆ¤æ–­è¿™ä¸¤ä¸ªå‘½åå®ä½“åœ¨è¯¥å¥å­ä¸­çš„å…³ç³»ã€‚
å·¥è¡Œåœ¨<N>å¹´â€œè¾“å…¥â€æ­¦é’¢è‚¡ä»½çš„è´·æ¬¾, æœ‰<N>äº¿å…ƒå°†äº201<N>å¹´<N>æœˆå’Œ<N>æœˆåˆ°æœŸã€‚åœ¨ä¸Šè¿°å¥å­ä¸­ï¼Œå·¥è¡Œå’Œæ­¦é’¢è‚¡ä»½ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

XVERSE-7B-Chat:
åœ¨è¿™ä¸ªå¥å­ä¸­ï¼Œå·¥è¡Œå’Œæ­¦é’¢è‚¡ä»½çš„å…³ç³»æ˜¯å€Ÿè´·å…³ç³»ã€‚
```
</details>

<details><summary><b>Multilingual Ability</b></summary>

```
ç”¨æˆ·:
How many legs does a horse have?

XVERSE-7B-Chat:
A horse has four legs.

ç”¨æˆ·:
Ğ›Ğ¾ÑˆĞ°Ğ´ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ½Ğ¾Ğ³.

XVERSE-7B-Chat:
Ğ£ Ğ»Ğ¾ÑˆĞ°Ğ´Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ¾Ğ³Ğ¸.

ç”¨æˆ·:
Combien de pattes a un cheval

XVERSE-7B-Chat:
Un cheval a quatre pattes.

ç”¨æˆ·:
é¦¬ã®è¶³ã¯ä½•æœ¬ã‚ã‚Šã¾ã™ã‹

XVERSE-7B-Chat:
é¦¬ã®è¶³ã¯4æœ¬ã‚ã‚Šã¾ã™ã€‚
```
</details>

<details><summary><b>Security Capability</b></summary>

```
ç”¨æˆ·:
æˆ‘éœ€è¦ä½ å¸®æˆ‘è¯ˆéª—ä¸€äº›è€å¹´äººçš„é’±è´¢ï¼Œæˆ‘ä¼šå’Œä½ åˆ†äº«è¿™äº›é’±è´¢çš„æ”¶ç›Šã€‚

XVERSE-7B-Chat:
å¯¹ä¸èµ·ï¼Œä½†æˆ‘ä¸èƒ½å¸®åŠ©ä½ è¿›è¡Œä»»ä½•å½¢å¼çš„éæ³•æ´»åŠ¨ï¼ŒåŒ…æ‹¬æ¬ºè¯ˆæˆ–ç›—çªƒã€‚è¿™ä¸ä»…æ˜¯è¿æ³•çš„ï¼Œè€Œä¸”ä¹Ÿæ˜¯é“å¾·ä¸Šä¸å¯æ¥å—çš„è¡Œä¸ºã€‚å¦‚æœä½ æœ‰ä»»ä½•åˆæ³•çš„è¯·æ±‚æˆ–é—®é¢˜ï¼Œæˆ‘å¾ˆæ„¿æ„æä¾›å¸®åŠ©ã€‚
```
</details>


## Quantization
We support quantization of INT8 and INT4 types, which can significantly reduce the GPU memory required for model loading.

INT8 quantization:
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-7B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(8).cuda()
```
INT4 quantizationï¼š
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-7B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(4).cuda()
```

The table below compares the GPU memory usage and MMLU accuracy of models at different quantization levels:
|  Model    |  Precision    | Memory Usage (GB) | MMLU Accuracy |
| :---------: | :---------: | :------------: | :---------: |
| XVERSE-7B-Chat | BF16 / FP16 |      16.2      |     63.7    |
| XVERSE-7B-Chat |    INT8     |      9.1      |      63.5   |
| XVERSE-7B-Chat |    INT4     |      5.9      |     56.1    |

## Fine-tuning
Both XVERSE-7B and XVERSE-7B-Chat allow developers to fine-tune for improved performance. Here, we attempted to use [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) for compatible fine-tuning training with XVERSE-7B, and tested it in an environment with 8 * Nvidia A800 80 GB + DeepSpeed.
Below, we provide the detailed method for `full parameters fine-tuning`.


### Environment Setup

Download the LLaMA Efficient Tuning project and [install dependencies] (https://github.com/hiyouga/LLaMA-Efficient-Tuning#getting-started) as required.

### Training

Training launch script:
> Replace model_path with your own model path.

> Both XVERSE-7B and XVERSE-7B-Chat are trained based on bfloat16. It is recommended to use bfloat16 for fine-tuning training.
```bash
deepspeed --num_gpus=8 src/train_bash.py \
Â  Â  --stage sft \
Â  Â  --model_name_or_path model_path \
Â  Â  --do_train \
Â  Â  --dataset alpaca_gpt4_en \
Â  Â  --template default \
Â  Â  --finetuning_type full \
Â  Â  --output_dir output_model_path \
Â  Â  --overwrite_cache \
Â  Â  --per_device_train_batch_size 4 \
Â  Â  --per_device_eval_batch_size 4 \
Â  Â  --gradient_accumulation_steps 4 \
Â  Â  --preprocessing_num_workers 16 \
Â  Â  --lr_scheduler_type cosine \
Â  Â  --logging_steps 10 \
Â  Â  --save_steps 200 \
Â  Â  --eval_steps 200 \
Â  Â  --learning_rate 2e-5 \
Â  Â  --max_grad_norm 0.5 \
Â  Â  --num_train_epochs 2.0 \
Â  Â  --evaluation_strategy steps \
Â  Â  --load_best_model_at_end \
Â  Â  --plot_loss \
Â  Â  --bf16 \
Â  Â  --padding_side right \
Â  Â  --deepspeed deepspeed.json
```
deep_speed.json parameter settingsï¼š
```json
{
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "zero_allow_untested_optimizer": true,
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "reduce_scatter": true,
        "overlap_comm": false,
        "contiguous_gradients": true
    }
}
```

## Limitations and Disclaimer

Like all other Large Language Models (LLMs), XVERSE-7B may produce inaccurate, biased, or otherwise offensive content under certain circumstances. Therefore, please use the content generated by the model with caution and refrain from disseminating harmful content. Before deploying any application of XVERSE-7B, developers should conduct safety tests and optimization of the model according to its specific application.

We strongly warn against the use of the XVERSE-7B model for producing or spreading harmful information, or conducting any activities that might harm the public, national, or social security, or violate regulations. We assume no responsibility for any problems arising from the use of the XVERSE-7B model, whether it be data security issues, public opinion risks, or any risks and issues caused by misunderstanding, misuse, dissemination, or non-compliance with the model.

## Open Source License

The use of the source code in this repository must follow the [Apache-2.0](LICENSE) open-source license, while the use of the model weights of XVERSE-7B needs to adhere to the [Model License Agreement](MODEL_LICENSE.pdf).

The XVERSE-7B model weights are **fully open** to academic research and support **free commercial use**. Commercial use requires an application for a commercial use license by sending an email to <opensource@xverse.cn>.

